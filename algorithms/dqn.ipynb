{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T13:24:11.859610Z",
     "start_time": "2024-11-01T13:24:11.856604Z"
    }
   },
   "source": [
    "from model import AwaleNetwork\n",
    "import jax\n",
    "from awale.env import AwaleJAX\n",
    "import equinox as eqx\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T13:19:45.501825Z",
     "start_time": "2024-11-01T13:19:45.430108Z"
    }
   },
   "cell_type": "code",
   "source": "env = AwaleJAX( )",
   "id": "f58d617eea95722e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T13:19:47.365052Z",
     "start_time": "2024-11-01T13:19:47.361574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "state = env.reset(key)"
   ],
   "id": "5aa291ca18da3dc2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T13:44:47.523871Z",
     "start_time": "2024-11-01T13:44:47.517737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNAgent :\n",
    "    def __init__(self , key) :\n",
    "        self.memory = [ ]\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = AwaleNetwork(key)\n",
    "\n",
    "    def remember(self , state , action , reward , next_state , done) :\n",
    "        memory_size = 2000\n",
    "        experience = (state , action , reward , next_state , done)\n",
    "        self.memory.append(experience)\n",
    "        if len(self.memory) > memory_size :\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def act(self , key , state , action_space) :\n",
    "        if random.uniform(key) <= self.epsilon :\n",
    "            return random.choice(key , action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return jnp.argmax(q_values[ 0 ])\n",
    "\n",
    "    def replay(self , batch_size , key) :\n",
    "        # Generate random indices using JAX\n",
    "        indices = jax.random.permutation(key , len(self.memory))[ :batch_size ]\n",
    "        minibatch = [ self.memory[ int(idx) ] for idx in indices ]\n",
    "        for state , action , reward , next_state , done in minibatch :\n",
    "            target = reward\n",
    "            if not done :\n",
    "                print(self.model(next_state.board , next_state.action_space , next_state.score))\n",
    "                target = reward + self.gamma * jnp.amax(self.model(next_state.board , next_state.action_space , next_state.score))\n",
    "                print(target)\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[ 0 ][ action ] = target\n",
    "            self.model.fit(state , target_f , epochs=1 , verbose=0)\n",
    "        if self.epsilon > self.epsilon_min :\n",
    "            self.epsilon *= self.epsilon_decay"
   ],
   "id": "2edd64d4a09aecde",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T11:02:15.552918Z",
     "start_time": "2024-11-01T11:02:15.443713Z"
    }
   },
   "cell_type": "code",
   "source": "jax.random.choice(key , jnp.array([ 1 , 2 , 3 ]))",
   "id": "f0e69b160d171201",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T13:44:48.807706Z",
     "start_time": "2024-11-01T13:44:48.802241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the DQN agent\n",
    "agent = DQNAgent(key)"
   ],
   "id": "aac6a27037cff193",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:54:40.150001Z",
     "start_time": "2024-11-01T10:54:40.144649Z"
    }
   },
   "cell_type": "code",
   "source": "agent.model",
   "id": "239abb83f0f98fcb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AwaleNetwork(\n",
       "  state_encoder=[\n",
       "    Linear(\n",
       "      weight=f32[64,14],\n",
       "      bias=f32[64],\n",
       "      in_features=14,\n",
       "      out_features=64,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[128,64],\n",
       "      bias=f32[128],\n",
       "      in_features=64,\n",
       "      out_features=128,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    <wrapped function relu>,\n",
       "    Linear(\n",
       "      weight=f32[64,128],\n",
       "      bias=f32[64],\n",
       "      in_features=128,\n",
       "      out_features=64,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    <wrapped function relu>\n",
       "  ],\n",
       "  action_embedding=Embedding(\n",
       "    num_embeddings=12,\n",
       "    embedding_size=32,\n",
       "    weight=f32[12,32]\n",
       "  ),\n",
       "  combine=[\n",
       "    Linear(\n",
       "      weight=f32[64,96],\n",
       "      bias=f32[64],\n",
       "      in_features=96,\n",
       "      out_features=64,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    <wrapped function relu>,\n",
       "    Linear(\n",
       "      weight=f32[32,64],\n",
       "      bias=f32[32],\n",
       "      in_features=64,\n",
       "      out_features=32,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    <wrapped function relu>,\n",
       "    Linear(\n",
       "      weight=f32[1,32],\n",
       "      bias=f32[1],\n",
       "      in_features=32,\n",
       "      out_features=1,\n",
       "      use_bias=True\n",
       "    )\n",
       "  ]\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T13:19:53.003845Z",
     "start_time": "2024-11-01T13:19:53.000778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "batch_size = 32\n",
    "num_episodes = 1000"
   ],
   "id": "cd78701e50626a8d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T13:44:54.663236Z",
     "start_time": "2024-11-01T13:44:54.600254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for episode in range(num_episodes) :\n",
    "    # random initial state\n",
    "    key = jax.random.PRNGKey(episode)\n",
    "    state = env.reset(key)\n",
    "    for time in range(500) :\n",
    "        # Choose action\n",
    "        key = jax.random.PRNGKey(time)\n",
    "        action = agent.act(key , state.board , state.action_space)\n",
    "        # Take action\n",
    "        next_state , reward , done = env.step(state , action)\n",
    "        # Remember the experience\n",
    "        agent.remember(state , action , reward , next_state , done)\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "        # Check if episode is finished\n",
    "        if done :\n",
    "            break\n",
    "        # Train the agent\n",
    "        print(len(agent.memory))\n",
    "        if len(agent.memory) > batch_size :\n",
    "            agent.replay(batch_size , key)\n"
   ],
   "id": "718f4207829ade36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "[0.11819653 0.13046151 0.03171215 0.13640115 0.03101879]\n",
      "-4.370419\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AwaleNetwork' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(agent\u001B[38;5;241m.\u001B[39mmemory))\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(agent\u001B[38;5;241m.\u001B[39mmemory) \u001B[38;5;241m>\u001B[39m batch_size :\n\u001B[0;32m---> 21\u001B[0m     \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[21], line 33\u001B[0m, in \u001B[0;36mDQNAgent.replay\u001B[0;34m(self, batch_size, key)\u001B[0m\n\u001B[1;32m     31\u001B[0m     target \u001B[38;5;241m=\u001B[39m reward \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m*\u001B[39m jnp\u001B[38;5;241m.\u001B[39mamax(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(next_state\u001B[38;5;241m.\u001B[39mboard , next_state\u001B[38;5;241m.\u001B[39maction_space , next_state\u001B[38;5;241m.\u001B[39mscore))\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28mprint\u001B[39m(target)\n\u001B[0;32m---> 33\u001B[0m target_f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m(state)\n\u001B[1;32m     34\u001B[0m target_f[ \u001B[38;5;241m0\u001B[39m ][ action ] \u001B[38;5;241m=\u001B[39m target\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mfit(state , target_f , epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m , verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'AwaleNetwork' object has no attribute 'predict'"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76d6d5315ccb6760"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
